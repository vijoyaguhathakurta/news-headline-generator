{"cells":[{"cell_type":"markdown","metadata":{"id":"Fz8yQwwPM3xI"},"source":["# Installing Packages needed and Importing Libraries"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1721726418941,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"YG8N0r5OM3xL"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import string\n","\n","import os\n","import time\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"L9sdceSdM3xN"},"source":["# Dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1414,"status":"ok","timestamp":1721726420342,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"_hiteNeAjNxG"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{"id":"QghgidKwkXuA"},"source":["###Loading the dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":10892,"status":"ok","timestamp":1721726431230,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"c_Hr9xWHM3xO","outputId":"1b85e9e8-e086-48be-9755-26a4b709eb27"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"news"},"text/html":["\n","  <div id=\"df-a87342a1-7292-4e17-a3e5-29ff266a980f\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Headline</th>\n","      <th>Content</th>\n","      <th>News Categories</th>\n","      <th>Date</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Congress leader Baljinder Singh shot dead at h...</td>\n","      <td>Congress leader Baljinder Singh was shot dead ...</td>\n","      <td>['national']</td>\n","      <td>19-09-2023</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>17-year-old girl preparing for NEET dies by su...</td>\n","      <td>Another NEET aspirant died by suicide in Rajas...</td>\n","      <td>['national']</td>\n","      <td>19-09-2023</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Hampers to welcome MPs in new Parliament tomor...</td>\n","      <td>In order to mark the first-ever working day of...</td>\n","      <td>['national']</td>\n","      <td>19-09-2023</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Only 10% women lawmakers in RS, while only 14%...</td>\n","      <td>Congress President Mallikarjun Kharge, while s...</td>\n","      <td>['national']</td>\n","      <td>19-09-2023</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Ganesh temple decorated with notes, coins wort...</td>\n","      <td>The Sri Sathya Ganapathi Temple in Bengaluru a...</td>\n","      <td>['national']</td>\n","      <td>19-09-2023</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a87342a1-7292-4e17-a3e5-29ff266a980f')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a87342a1-7292-4e17-a3e5-29ff266a980f button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a87342a1-7292-4e17-a3e5-29ff266a980f');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-480ffda7-2276-47f6-b0c8-46ea2fa415b9\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-480ffda7-2276-47f6-b0c8-46ea2fa415b9')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-480ffda7-2276-47f6-b0c8-46ea2fa415b9 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["                                            Headline  \\\n","0  Congress leader Baljinder Singh shot dead at h...   \n","1  17-year-old girl preparing for NEET dies by su...   \n","2  Hampers to welcome MPs in new Parliament tomor...   \n","3  Only 10% women lawmakers in RS, while only 14%...   \n","4  Ganesh temple decorated with notes, coins wort...   \n","\n","                                             Content News Categories  \\\n","0  Congress leader Baljinder Singh was shot dead ...    ['national']   \n","1  Another NEET aspirant died by suicide in Rajas...    ['national']   \n","2  In order to mark the first-ever working day of...    ['national']   \n","3  Congress President Mallikarjun Kharge, while s...    ['national']   \n","4  The Sri Sathya Ganapathi Temple in Bengaluru a...    ['national']   \n","\n","         Date  \n","0  19-09-2023  \n","1  19-09-2023  \n","2  19-09-2023  \n","3  19-09-2023  \n","4  19-09-2023  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["news = pd.read_csv(\"news.csv\",engine=\"python\",on_bad_lines='skip')\n","news.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1721726431231,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"S8WshXDklqnq","outputId":"8065a46e-db85-4576-9b59-726068f60773"},"outputs":[{"name":"stdout","output_type":"stream","text":["The total no. of samples in the dataset = 270474\n"]}],"source":["print (\"The total no. of samples in the dataset =\",news.shape[0])"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1721726431231,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"hYOTv8AING0u"},"outputs":[],"source":["text = news['Content']\n","summary = news['Headline']"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1721726431231,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"FE1BN_FCgdkU"},"outputs":[],"source":["# using the train test split function\n","text, text_test, summary, summary_test = train_test_split(text, summary, random_state=104, test_size=0.1, shuffle=True)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1721726431231,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"vqhuG5tTmLxH","outputId":"e3c1f625-9d9b-45ac-a833-ce92db929b80"},"outputs":[{"name":"stdout","output_type":"stream","text":["The no. of training samples = 243426\n"]}],"source":["print (\"The no. of training samples =\",text.shape[0])"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":747,"status":"ok","timestamp":1721726431968,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"uaSyu28HmWYU","outputId":"80637f66-2520-4170-9a3f-7a2cf7a5aed1"},"outputs":[{"name":"stdout","output_type":"stream","text":["The no. of test samples = 27048\n"]}],"source":["print (\"The no. of test samples =\",text_test.shape[0])"]},{"cell_type":"markdown","metadata":{"id":"twghru39kcZp"},"source":["###Preprocessing\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1721726431968,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"514PkbRsM3xP"},"outputs":[],"source":["import contractions\n","import re\n","def preprocess(x):\n","  # expand contractions\n","  x= x.apply(lambda y: contractions.fix(y))\n","  # remove html tags\n","  x= x.apply(lambda y: re.compile('<.*?>').sub(r'',y))\n","  # remove url\n","  x= x.apply(lambda y: re.compile(r'https?://\\S+|www\\.\\S+').sub(r'',y))\n","  # remove 's\n","  x=  x.apply(lambda y: re.sub(r\"'s\\b\",\"\",y))\n","  # remove '\n","  x= x.apply(lambda y: re.sub(\"'\",'', y))\n","  # add end and start token\n","  x= x.apply(lambda y: 'sos ' + y + ' eos')\n","  return x\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":18906,"status":"ok","timestamp":1721726450865,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"TDhZ7DNGM3xQ"},"outputs":[],"source":["text=preprocess(text)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":7794,"status":"ok","timestamp":1721726458634,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"cnlxrI3oM3xR"},"outputs":[],"source":["summary=preprocess(summary)"]},{"cell_type":"markdown","metadata":{"id":"RfY0GKbWaw4G"},"source":["###Tokenization"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":46157,"status":"ok","timestamp":1721726504761,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"AScm9KJAYWLa"},"outputs":[],"source":["import tensorflow as tf\n","\n","# convert to lowercase,remove punctuations, tokenize\n","\n","# create tokenizer object\n","tokenizer=tf.keras.preprocessing.text.Tokenizer(oov_token=\"<unk>\")\n","\n","# fit tokenizer on data\n","tokenizer.fit_on_texts(text)\n","tokenizer.fit_on_texts(summary)\n","\n","# convert text to numbers\n","text=tokenizer.texts_to_sequences(text)\n","summary=tokenizer.texts_to_sequences(summary)\n","\n","# vocabulary\n","VOCAB=tokenizer.word_index\n","VOCAB_SIZE=len(VOCAB)+1\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":1237,"status":"ok","timestamp":1721726505993,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"i0s7YoYQPf0B"},"outputs":[],"source":["import pickle\n","# saving\n","with open('/content/drive/My Drive/tokenizer.pickle', 'wb') as handle:\n","    pickle.dump(tokenizer, handle)\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1721726505993,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"1JaA9mGrM3xM"},"outputs":[],"source":["ENCODER_LEN = 100\n","DECODER_LEN = 20"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":1755,"status":"ok","timestamp":1721726507746,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"6teg6cNmUWu1"},"outputs":[],"source":["# post padding the sequences so that they are of same size\n","text=tf.keras.utils.pad_sequences(text, maxlen=ENCODER_LEN, truncating='post', padding='post')\n","summary=tf.keras.utils.pad_sequences(summary, maxlen=DECODER_LEN, truncating='post', padding='post')\n"]},{"cell_type":"markdown","metadata":{"id":"L8GaaXrLkhkE"},"source":["###Preparing the dataset for training (Making batches)"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1721726507746,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"RKpsgIXPSLKG"},"outputs":[],"source":["BATCH_SIZE = 64\n","BUFFER_SIZE = BATCH_SIZE*8"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":677,"status":"ok","timestamp":1721726508419,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"z0H3Lo1mM3xR"},"outputs":[],"source":["text = tf.cast(text, dtype=tf.int64)\n","summary = tf.cast(summary, dtype=tf.int64)"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1721726508419,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"uMyHTEJCM3xT"},"outputs":[],"source":["dataset = tf.data.Dataset.from_tensor_slices((text,summary)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"]},{"cell_type":"markdown","metadata":{"id":"a13apo9BM3xT"},"source":["# DESIGNING TRANSFORMER"]},{"cell_type":"markdown","metadata":{"id":"2V5nLbGjmiYi"},"source":["### Positional Encoding function"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1721726508419,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"z1cvnw_PM3xU"},"outputs":[],"source":["# The code implements the formula, but instead of interleaving the sines and cosines,\n","# the vectors of sines and cosines are simply concatenated.\n","# Permuting the channels like this is functionally equivalent, and just a little easier to implement.\n","\n","def positional_encoding(length, depth):\n","  # length: length of the sequence\n","  # depth: dimension of the output embedding space\n","\n","  # positions: positions of a token in input sequence\n","  positions = np.arange(length)[:, np.newaxis]\n","\n","  # i is used for mapping to column indices [0<=i<=depth/2]\n","  i = depth/2\n","  i = np.arange(i)[np.newaxis, :]\n","\n","  # Angle in radians as per the formula\n","  angle_rates = 1 / (10000**(2*i/depth))\n","  angle_rads = positions * angle_rates\n","\n","  # Concatenating the sines and cos\n","  pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n","\n","  # return the result after casting in to compatible datatype\n","  return tf.cast(pos_encoding, dtype=tf.float32)"]},{"cell_type":"markdown","metadata":{"id":"LXNK1lG1oaQt"},"source":["###ATTENTION layer"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1721726508419,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"hTcjOmXlM3xU"},"outputs":[],"source":["class MultiHead_Attention(tf.keras.layers.Layer):\n","# extending the keras Layer class to create a new class\n","\n","    def __init__(self, d_model, num_heads):\n","        # calling the super class(Layer) initializer\n","        super(MultiHead_Attention, self).__init__()\n","\n","        # Attributes of MultiHead_Attention\n","\n","        # num_heads: no. of heads in the multihead attention\n","        self.num_heads = num_heads\n","\n","        # d_model: dimension of the model\n","        self.d_model = d_model\n","\n","        # if d_model % num_heads not equal to 0 raise AssertionError\n","        assert d_model % self.num_heads == 0\n","\n","        # depth: dimension of each head\n","        self.depth = d_model // self.num_heads\n","\n","        # Linear dense layers of d_model units,\n","        # to produce query\n","        self.wq = tf.keras.layers.Dense(d_model)\n","        # to produce key\n","        self.wk = tf.keras.layers.Dense(d_model)\n","        # to produce value\n","        self.wv = tf.keras.layers.Dense(d_model)\n","\n","        # the last linear layer of the multihead-attention\n","        self.dense = tf.keras.layers.Dense(d_model)\n","\n","\n","    def split_heads(self, x, batch_size):\n","        # splits the sequence of d_model dimension to num_heads no. of depth dimension tensors\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    # Scaled Dot-Product Attention\n","    def scaled_dot_product_attention(self,q, k, v, mask):\n","\n","      # matrix multiplication of the query and transpose(key)\n","      matmul_qk = tf.matmul(q, k, transpose_b=True)\n","\n","      # dk: dimension of key\n","      dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","\n","      # Scaling\n","      scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","      # if token is to be masked its attention logits is set to -infinity\n","      if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)\n","\n","      # softmax on the attention logits to get the attention weights\n","      attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n","\n","      # matrix multiplication of the resulting attention weights and value\n","      output = tf.matmul(attention_weights, v)\n","\n","      return output, attention_weights\n","\n","\n","    def call(self, v, k, q, mask):\n","\n","        batch_size = tf.shape(q)[0]\n","\n","        # the linear transformations to get the query,key,value from the inputs\n","        q = self.wq(q)\n","        k = self.wk(k)\n","        v = self.wv(v)\n","\n","        # splitting each of query,key,value into multiple heads\n","        q = self.split_heads(q, batch_size)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","\n","        # calculating the Scaled Dot-Product Attention\n","        scaled_attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)\n","\n","        # changing the output to compatible type\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","\n","        # concatinating the attention outputs\n","        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n","\n","        # passing the output through  the final linear layer\n","        output = self.dense(concat_attention)\n","\n","        return output, attention_weights\n"]},{"cell_type":"markdown","metadata":{"id":"Z-xv1JSyt40i"},"source":["###Feed-forward Neural Network"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1721726508420,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"iHLVOF8eORjJ"},"outputs":[],"source":["def feed_forward_network(d_model, dff):\n","    # creates a  2 layer neural network with ReLU activation function\n","    return tf.keras.Sequential([\n","        tf.keras.layers.Dense(dff, activation='relu'),\n","        tf.keras.layers.Dense(d_model)  ])"]},{"cell_type":"markdown","metadata":{"id":"9cR8Dztwt_MV"},"source":["### Encoder layer"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1721726508420,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"WLdpyLyJM3xV"},"outputs":[],"source":["# A SINGLE ENCODER LAYER\n","class EncoderLayer(tf.keras.layers.Layer):\n","# extending the keras Layer class to create a new class\n","\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","         # calling the super class(Layer) initializer\n","        super(EncoderLayer, self).__init__()\n","\n","        # Multihead attention layer\n","        self.mha = MultiHead_Attention(d_model, num_heads)\n","\n","        # Point-wise feed forward neural network(FFN)\n","        self.ffn = feed_forward_network(d_model, dff)\n","\n","        # layer normalization for attention layer\n","        self.layernorm_mha = tf.keras.layers.LayerNormalization()\n","        # layer normalization for FFN layer\n","        self.layernorm_ffn = tf.keras.layers.LayerNormalization()\n","\n","        # dropout layer for attention layer\n","        self.dropout_mha = tf.keras.layers.Dropout(rate)\n","        # dropout layer for FFN layer\n","        self.dropout_ffn = tf.keras.layers.Dropout(rate)\n","\n","\n","    def call(self, x, training, mask):\n","\n","        # input x is passed to Self-Attention layer\n","        #(all of the keys, values and queries come from the same place, in this case,\n","        # the output of the previous layer in the encoder)\n","        attn_output, _ = self.mha(x, x, x, mask)\n","        # applying dropout to this output\n","        attn_output = self.dropout_mha(attn_output, training=training)\n","        # add the residual connection and layer normalize\n","        out1 = self.layernorm_mha(x + attn_output)\n","\n","        # the final attention output is feed to FFN\n","        ffn_output = self.ffn(out1)\n","        # applying dropout to this output\n","        ffn_output = self.dropout_ffn(ffn_output, training=training)\n","        # add the residual connection and layer normalize\n","        out2 = self.layernorm_ffn(out1 + ffn_output)\n","\n","        return out2"]},{"cell_type":"markdown","metadata":{"id":"FF15c5Q-uxj2"},"source":["###Decoder Layer"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1721726508420,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"kdNHDajKM3xV"},"outputs":[],"source":["# A SINGLE DECODER LAYER\n","class DecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(DecoderLayer, self).__init__()\n","\n","        # 2 Multihead attention layers (one for decoder and another is encoder-decoder layer )\n","        self.masked_mha = MultiHead_Attention(d_model, num_heads)\n","        self.cross_mha = MultiHead_Attention(d_model, num_heads)\n","\n","        # Point-wise feed forward neural network(FFN)\n","        self.ffn = feed_forward_network(d_model, dff)\n","\n","        # layer normalization for both attention layer\n","        self.layernorm_masked_mha = tf.keras.layers.LayerNormalization()\n","        self.layernorm_cross_mha = tf.keras.layers.LayerNormalization()\n","\n","        # layer normalization for FFN layer\n","        self.layernorm_ffn = tf.keras.layers.LayerNormalization()\n","\n","        # dropout layer for attention layer\n","        self.dropout_masked_mha = tf.keras.layers.Dropout(rate)\n","        self.dropout_cross_mha = tf.keras.layers.Dropout(rate)\n","\n","        # dropout layer for FFN layer\n","        self.dropout_ffn = tf.keras.layers.Dropout(rate)\n","\n","\n","    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n","\n","        # The masked self-attention layers in the decoder allow each position in the decoder\n","        # to attend to all positions in the decoder up to and including that position\n","        masked_attn, masked_attn_weights = self.masked_mha(x, x, x, look_ahead_mask)\n","        # applying the dropout\n","        masked_attn = self.dropout_masked_mha(masked_attn, training=training)\n","\n","        # add the residual connection and layer normalize\n","        out1 = self.layernorm_masked_mha(masked_attn + x)\n","\n","        # the queries come from the previous decoder layer,\n","        # and the memory keys and values come from the output of the encoder\n","        cross_attn, cross_attn_weights = self.cross_mha(enc_output, enc_output, out1, padding_mask)\n","        # applying the dropout\n","        cross_attn = self.dropout_cross_mha(cross_attn, training=training)\n","\n","        # add the residual connection and layer normalize\n","        out2 = self.layernorm_cross_mha(cross_attn + out1)\n","\n","        # the final attention output is feed to FFN\n","        ffn_output = self.ffn(out2)\n","        # applying dropout to this output\n","        ffn_output = self.dropout_ffn(ffn_output, training=training)\n","        # add the residual connection and layer normalize\n","        out3 = self.layernorm_ffn(ffn_output + out2)\n","\n","        return out3, masked_attn_weights, cross_attn_weights"]},{"cell_type":"markdown","metadata":{"id":"MGK2SAdExIlL"},"source":["### ENCODER"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1721726508420,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"75FMlz2nM3xW"},"outputs":[],"source":["# The ENCODER with N encoder layers\n","class Encoder(tf.keras.layers.Layer):\n","\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n","        super(Encoder, self).__init__()\n","\n","        # d_model: dimensions of the model\n","        self.d_model = d_model\n","        # num_layers: No. of encoder layers(N)\n","        self.num_layers = num_layers\n","\n","        # Embedding layer\n","        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","\n","        # Positional-encoding layer\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n","\n","        # a stack of N layers\n","        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","\n","        # Dropout layer\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, x, training, mask):\n","\n","        # seq_len: length of the token sequences in x\n","        seq_len = tf.shape(x)[1]\n","\n","        # passing the input x to the embedding layer\n","        x = self.embedding(x)\n","        # This factor sets the relative scale of the embedding and positonal_encoding.\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        # adding the embedded vector and the positional encoding\n","        x = x + self.pos_encoding[tf.newaxis, :seq_len, :]\n","\n","        # applying dropout on x\n","        x = self.dropout(x, training=training)\n","\n","        # feeding x to the stack of encoder layers\n","        for i in range(self.num_layers):\n","            x = self.enc_layers[i](x, training, mask)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"RPSPYQJbxUik"},"source":["###DECODER"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1721726508420,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"k8dzplIExTR4"},"outputs":[],"source":["# The DECODER with N decoder layers\n","class Decoder(tf.keras.layers.Layer):\n","\n","    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n","        super(Decoder, self).__init__()\n","\n","        # d_model: dimensions of the model\n","        self.d_model = d_model\n","        # num_layers: No. of encoder layers(N)\n","        self.num_layers = num_layers\n","\n","        # Embedding layer\n","        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","\n","        # Positional-encoding layer\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","\n","        # a stack of N decoder layers\n","        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","\n","        # Dropout layer\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n","        # seq_len: length of the token sequences in x\n","        seq_len = tf.shape(x)[1]\n","        attention_weights = {}\n","\n","        # passing the input x to the embedding layer\n","        x = self.embedding(x)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        # adding the embedded vector and the positional encoding\n","        x = x + self.pos_encoding[tf.newaxis, :seq_len, :]\n","\n","        # applying dropout on x\n","        x = self.dropout(x, training=training)\n","\n","        # feeding x to the stack of decoder layers\n","        for i in range(self.num_layers):\n","            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n","            # This constructs a dynamic key for accessing attention weights within a specific decoder layer.\n","            # The {} is a placeholder that will be replaced with the actual value of i+1.\n","            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n","            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n","\n","        return x, attention_weights\n"]},{"cell_type":"markdown","metadata":{"id":"rjx70tbr3Jhl"},"source":["## TRANSFORMER MODEL"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1721726508420,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"POXJBOnyM3xW"},"outputs":[],"source":["class Transformer(tf.keras.Model):\n","# creating a model\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size,rate=0.1):\n","        # calling the super class(Model) initializer\n","        super(Transformer, self).__init__()\n","\n","        # Encoder\n","        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, rate)\n","\n","        # Decoder\n","        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, rate)\n","\n","        # Final Linear layer\n","        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","\n","    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n","\n","        # pass the input and the padding mask to encoder\n","        enc_output = self.encoder(inp, training, enc_padding_mask)\n","\n","        # pass the encoder output, look ahead mask for decoder masked self attention,\n","        # padding mask for the encoder-decoder attention to the decoder\n","        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n","\n","        # pass the decoder output to the linear layer\n","        final_output = self.final_layer(dec_output)\n","\n","        return final_output, attention_weights"]},{"cell_type":"markdown","metadata":{"id":"8aWw5nHwM3xX"},"source":["# TRAINING THE MODEL"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1721726508420,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"bMFHxIfyn_LV"},"outputs":[],"source":["num_layers = 3  # original=6\n","d_model = 128 # original=512\n","dff = 512 # original=2048\n","num_heads = 4 # original=8\n","dropout_rate = 0.1 # original= 0.1\n","EPOCHS = 20"]},{"cell_type":"markdown","metadata":{"id":"i3od6q8K3fEh"},"source":["###Custom Learning Rate"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1721726508420,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"oHj9iehyM3xX"},"outputs":[],"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","  def __init__(self, d_model, warmup_steps=4000):\n","    super().__init__()\n","\n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","    self.warmup_steps = warmup_steps\n","\n","  def __call__(self, step):\n","    step = tf.cast(step, dtype=tf.float32)\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps ** -1.5)\n","\n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"]},{"cell_type":"markdown","metadata":{"id":"FMNYzSQe3oTk"},"source":["###Adam Optimiser"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1721726508421,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"-FOIljQeM3xX"},"outputs":[],"source":["learning_rate = CustomSchedule(d_model)\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"]},{"cell_type":"markdown","metadata":{"id":"syM4Qa5sM3xY"},"source":["### Masked Loss"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1721726508421,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"1Agk8SdGM3xY"},"outputs":[],"source":["def loss_function(label, pred):\n","\n","  # creates a boolean mask where the label is not zero\n","  mask = label != 0\n","\n","  # initializes the loss function without reduction, meaning it will return the loss for each element.\n","  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","\n","  #computes the loss for each element\n","  loss = loss_object(label, pred)\n","\n","  # mask is cast to the same type as the loss and applied to zero out the loss where the mask is false.\n","  mask = tf.cast(mask, dtype=loss.dtype)\n","  loss *= mask\n","\n","  # loss is summed and then normalized by the sum of the mask to get the average loss over the non-masked elements\n","  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n","\n","  return loss"]},{"cell_type":"markdown","metadata":{"id":"d6AbOLId31QX"},"source":["### Masked Accuracy"]},{"cell_type":"markdown","metadata":{"id":"R3Ff1JDzLLm7"},"source":["Your accuracy_function computes the accuracy between predicted sequences (pred) and ground truth sequences (label) in a sequence-to-sequence task.\n","\n","The accuracy measure you’ve implemented is commonly known as Token Accuracy or Sequence Token Accuracy. It evaluates how well a sequence-to-sequence model predicts individual tokens in the output sequence compared to the ground truth tokens\n","\n","\n","First, you find the most likely token index for each time step in the predicted sequence using tf.argmax(pred, axis=2). This assumes that pred is a tensor with shape (batch_size, sequence_length, num_classes).\n","\n","Next, you cast the label tensor to the same data type as pred.\n","You create a boolean mask where each element is True if the label is non-zero (i.e., not a padding token).\n","\n","The match tensor contains True at positions where the predicted token matches the ground truth token and the label is non-zero. It’s computed as match = label == pred & mask.\n","\n","Finally, you calculate the accuracy by dividing the sum of match (correct predictions) by the sum of mask (total non-padding tokens).\n","\n","Overall, your function provides a way to evaluate the accuracy of sequence predictions while handling padding tokens appropriately.\n"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1721726508421,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"sBvfo82633iQ"},"outputs":[],"source":["def accuracy_function(label, pred):\n","\n","  label = tf.cast(label, pred.dtype)\n","\n","  # creates a boolean tensor where each element is True\n","  # if the corresponding elements in label and pred are equal\n","  match = label == pred\n","\n","  # creates a boolean mask where the label is not zero.\n","  mask = label != 0\n","  # applies the mask to the match tensor, setting elements to False where the mask is False\n","  match = match & mask\n","\n","  match = tf.cast(match, dtype=tf.float32)\n","  mask = tf.cast(mask, dtype=tf.float32)\n","\n","  # accuracy is calculated as the sum of the matches divided by the sum of the mask,\n","  # giving the proportion of correct predictions among the non-zero labels.\n","  return tf.reduce_sum(match)/tf.reduce_sum(mask)"]},{"cell_type":"markdown","metadata":{"id":"vr5ymYPz4Eso"},"source":["### Instance of transformer model"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1721726508421,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"QRkjuJVWM3xY"},"outputs":[],"source":["transformer = Transformer(\n","    num_layers= num_layers,\n","    d_model= d_model,\n","    num_heads= num_heads,\n","    dff= dff,\n","    input_vocab_size= VOCAB_SIZE,\n","    target_vocab_size= VOCAB_SIZE,\n","    rate= dropout_rate)"]},{"cell_type":"markdown","metadata":{"id":"UNCRgx4B4MCj"},"source":["### Creating Masks"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":494,"status":"ok","timestamp":1721726508908,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"vjQ82IPrN-TE"},"outputs":[],"source":["def create_padding_mask(seq):\n","    # creates a padding mask\n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","    return seq[:, tf.newaxis, tf.newaxis, :]\n","\n","def create_look_ahead_mask(size):\n","    # creates the look ahead mask\n","    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","    return mask\n"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1721726508909,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"bFuMMEvTM3xZ"},"outputs":[],"source":["def create_masks(inp, tar):\n","    # creates all the masks needed\n","    enc_padding_mask = create_padding_mask(inp)\n","    dec_padding_mask = create_padding_mask(inp)\n","\n","    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","    dec_target_padding_mask = create_padding_mask(tar)\n","    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","\n","    return enc_padding_mask, combined_mask, dec_padding_mask"]},{"cell_type":"markdown","metadata":{"id":"0Xj7rJpU4TUz"},"source":["### Train function"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1721726508909,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"Fu3L08cfM3xY"},"outputs":[],"source":["# finds the mean loss for each batch\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","# finds the mean training accuracy\n","train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1721726508909,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"OPNibuunM3xa"},"outputs":[],"source":["@tf.function\n","def train_step(inp, tar):\n","# function to train the model using teacher forcing\n","\n","    # the target sequence given to the decoder (the last token is removed)\n","    tar_inp = tar[:, :-1]\n","    # the target sequence used for finding the loss (the first token is removed)\n","    tar_real = tar[:, 1:]\n","\n","    # generating the masks needed\n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","\n","    # minimize the loss\n","    with tf.GradientTape() as tape:\n","        # output from the model\n","        predictions, _ = transformer(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n","        # loss between the output and real target sequence\n","        loss = loss_function(tar_real, predictions)\n","\n","    # updating the weights\n","    gradients = tape.gradient(loss, transformer.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","\n","    # finding mean loss\n","    train_loss(loss)\n","    # finding the mean accuracy\n","    train_accuracy(accuracy_function(tar_real, tf.argmax(predictions, axis=2)))"]},{"cell_type":"markdown","metadata":{"id":"ykGGEa83M3xa"},"source":["### Training the Model"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"elapsed":29386,"status":"error","timestamp":1721726538290,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"BtXe4MYAM3xb","outputId":"93f66079-1e8a-4a13-d3d7-1f766055f63e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1 Batch 0 Loss 11.2957 Accuracy 0.0000\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-caf27ec788be>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# train using the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for epoch in range(EPOCHS):\n","    start = time.time()\n","\n","    # resetting the mean loss before each training step\n","    train_loss.reset_states()\n","\n","    # for each batch in the training dataset\n","    for (batch, (inp, tar)) in enumerate(dataset):\n","        # train using the function\n","        train_step(inp, tar)\n","\n","        if batch % 100 == 0:\n","            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n","\n","    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n","    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n"]},{"cell_type":"markdown","metadata":{"id":"7HWQg0QAX1vL"},"source":["### Saving the model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":19,"status":"aborted","timestamp":1721726538292,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"CqqlMPGLHr4n"},"outputs":[],"source":["transformer.save_weights('/content/drive/My Drive/Project/checkpoints')\n"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15703,"status":"ok","timestamp":1721726558383,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"8m8IBSsPIEK2","outputId":"18613b75-ee89-4cd3-908e-78fc329a3d11"},"outputs":[{"data":{"text/plain":["<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x791a62074970>"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["transformer.load_weights('/content/drive/My Drive/checkpoints')"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1721726558384,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"CQD_fVSCHJyn"},"outputs":[],"source":["with open('/content/drive/My Drive/tokenizer.pickle', 'rb') as handle:\n","    tokenizer = pickle.load(handle)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":19,"status":"aborted","timestamp":1721726538293,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"oNItH-X5KZ9b"},"outputs":[],"source":["transformer.summary()"]},{"cell_type":"markdown","metadata":{"id":"nMUTjB6PUJn2"},"source":["#EVALUATION"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":490,"status":"ok","timestamp":1721726565518,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"c_ijp_ZGULVw"},"outputs":[],"source":["def evaluate(input, output):\n","# input is feed to the encoder\n","# initially only the \"sos\" token is feed to decoder i.e. the output\n","# generate tokens until the sequence is of DECODER_LEN\n","    for i in range(1,DECODER_LEN):\n","\n","        # generate the mask required\n","        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(input, output)\n","\n","        # the output probabilities are obtained from the model\n","        # training is set to False\n","        predictions, _ = transformer(input, output, False, enc_padding_mask, combined_mask, dec_padding_mask)\n","\n","        # predicted output probabilities for the last token (till now)\n","        predictions = predictions[: ,-1:, :]\n","\n","        # finding the token\n","        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","        # if this function is used for summarization and not testing\n","        # return output as soon as \"eos\" token is recieved\n","        if predicted_id.shape[0]==1 and predicted_id == tokenizer.word_index['eos']:\n","           return output\n","\n","        # add the last token to the output and feed this sequence to the decoder in the next iteration\n","        output = tf.concat([output, predicted_id], axis=-1)\n","\n","    return output"]},{"cell_type":"markdown","metadata":{"id":"wihTanCvld66"},"source":["#TESTING"]},{"cell_type":"code","execution_count":67,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":478},"executionInfo":{"elapsed":482,"status":"error","timestamp":1721727802168,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"yInWBdIVIwIO","outputId":"628c2fb8-f1de-4a6d-f001-4be2dd8e63f0"},"outputs":[{"ename":"KeyError","evalue":"86875","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 86875","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-67-0af14b5ee991>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m86875\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 86875"]}],"source":["text_test[86875]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1721726538293,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"ZD5SKrCcxUCP"},"outputs":[],"source":["# preprocess\n","text_test=preprocess(text_test)\n","summary_test=preprocess(summary_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1721726538293,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"LGkSop-_xUCR"},"outputs":[],"source":["# convert to lowercase,remove punctuations, tokenize\n","text_test=tokenizer.texts_to_sequences(text_test)\n","summary_test=tokenizer.texts_to_sequences(summary_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1721726538293,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"hGOqy7OUzRxy"},"outputs":[],"source":["# pad the sequences\n","text_test= tf.keras.utils.pad_sequences(text_test, maxlen=ENCODER_LEN, truncating='post', padding='post')\n","summary_test= tf.keras.utils.pad_sequences(summary_test, maxlen=DECODER_LEN, truncating='post', padding='post')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":19,"status":"aborted","timestamp":1721726538294,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"bIaviLUpxUCS"},"outputs":[],"source":["text_test = tf.cast(text_test, dtype=tf.int32)\n","summary_test = tf.cast(summary_test, dtype=tf.int32)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1721726538294,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"18EFJ41axUCS"},"outputs":[],"source":["# create batches of the testing set\n","dataset_test = tf.data.Dataset.from_tensor_slices((text_test,summary_test)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1721726538294,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"nZ0q6tGw0E6z"},"outputs":[],"source":["dataset_test"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1721726538294,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"KOU3vTbGNtNG"},"outputs":[],"source":["test_accuracy = tf.keras.metrics.Mean(name='test_accuracy')"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"executionInfo":{"elapsed":19,"status":"aborted","timestamp":1721726538295,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"Pg3pzDyezE1J"},"outputs":[],"source":["for (batch, (inp, tar)) in enumerate(dataset_test):\n","\n","  # passing the \"sos\" tokens\n","  out= tar[:,0:1]\n","  # generating the output sequences\n","  pred= evaluate(inp,out)\n","\n","  # mask the eos tokens\n","  pred= tf.where(tf.equal(pred, tokenizer.word_index['eos']), 0, pred)\n","  label= tf.where(tf.equal(tar, tokenizer.word_index['eos']), 0, tar)\n","\n","  # find the mean accuracy\n","  test_accuracy(accuracy_function(label,pred))\n","\n","  if batch % 10 == 0:\n","            print(f'Batch {batch}  Accuracy {test_accuracy.result():.4f}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":19,"status":"aborted","timestamp":1721726538295,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"JgmKJswgft6y"},"outputs":[],"source":["print(test_accuracy.result())"]},{"cell_type":"markdown","metadata":{"id":"sdJXdFETc7mt"},"source":["#Summarization"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":474,"status":"ok","timestamp":1721726572996,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"Mc5EyP6EUsH7"},"outputs":[],"source":["def summarize(txt):\n","\n","  # convert the txt string to sequence of tokens after preprocessing\n","  txt= preprocess(pd.Series(txt))\n","  txt= tokenizer.texts_to_sequences(txt)\n","  txt= tf.keras.utils.pad_sequences(txt, maxlen= ENCODER_LEN, truncating='post', padding='post')\n","  txt= tf.cast(txt, dtype=tf.int32)\n","\n","  # the initial output or decoder input - 'sos' token\n","  out= tf.cast([[tokenizer.word_index['sos']]],dtype=tf.int32)\n","\n","  # generate the output sequence\n","  p= evaluate(txt,out)\n","  p=p.numpy()\n","\n","  # convert output sequence to text\n","  p= tokenizer.sequences_to_texts(p)\n","\n","  # return the output text excluding the 'sos'\n","  return p[0][4:]"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2340,"status":"ok","timestamp":1721726755884,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"gTi1FtXkdbBd","outputId":"0078f528-00da-4d3c-92be-b8993a8ea8fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Article:  Radhamani Textiles, the owner of menswear range Rare Rabbit and other apparel brands, is working with investors to finalise funding at a valuation of $300-320 million, sources tell online publication The Arc. A91 Partners, a venture-capital fund, may lead the round. A91 already backs consumer brands like Atomberg and Sugar Cosmetics. Rare Rabbit operates at least 100 outlets.\n","Original Headline:  Fashion brand Rare Rabbit eyes funding at $300-mn valuation\n","Generated Headline:  fashion brand rare rabbit eyes funding at 300 mn valuation\n"]}],"source":["print(\"Article: \", news[\"Content\"][86875])\n","print(\"Original Headline: \", news[\"Headline\"][86875])\n","print(\"Generated Headline: \",summarize(news[\"Content\"][86875]))"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3342,"status":"ok","timestamp":1721726804536,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"r5O3ps4Ym-_F","outputId":"7fb8b44c-a62f-46b6-ef36-42367fd9d1dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Article:  Society of Manufacturers of Electric Vehicles has written a letter to the Minister of Heavy Industries (MHI) after seven electric two-wheeler makers were asked to return ₹469 crore for violating the FAME-II scheme's norms. \"Since...the subsidies passed on to customers...now stand cancelled...the customers who've taken such subsidies can be asked to return these...in all fairness,\" the letter said.\n","Original Headline:  Customers could be asked to return cancelled FAME subsidy: EV body\n","Generated Headline:  customers could be asked to return cancelled fame subsidy ev body\n"]}],"source":["print(\"Article: \", news[\"Content\"][111325])\n","print(\"Original Headline: \", news[\"Headline\"][111325])\n","print(\"Generated Headline: \",summarize(news[\"Content\"][111325]))"]},{"cell_type":"code","execution_count":65,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2614,"status":"ok","timestamp":1721727637460,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"Si57tEoFjLAS","outputId":"41ea7730-94d1-480e-c1bc-a60c4bb4a6c5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Article:  The National Testing Agency (NTA) released the examination calendar for major exams like JEE, NEET, CUET and UGC NET for the academic year 2024-25. JEE Main will be held in two sessions in January-February and in April. NEET and CUET UG will take place in May. Examination-specific details will be provided to candidates through the information bulletin of respective exams. \n","Original Headline:  Calendar for major exams like JEE, NEET for 2024-25 released\n","Generated Headline:  calendar for major exams like jee neet for 2024 25 released\n"]}],"source":["print(\"Article: \", news[\"Content\"][24664])\n","print(\"Original Headline: \", news[\"Headline\"][24664])\n","print(\"Generated Headline: \",summarize(news[\"Content\"][24664]))"]},{"cell_type":"code","execution_count":74,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2334,"status":"ok","timestamp":1721728322879,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"rAAeB_j_na4m","outputId":"c2d41008-9724-4c9e-9c96-b83b500fadf5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Article:  A new malaria vaccine, developed by researchers at the University of Oxford, has shown an efficacy rate of 77% in recent trials. This breakthrough could significantly reduce the global burden of malaria, which affects millions annually. The vaccine, named R21/Matrix-M, is expected to be a game-changer in regions heavily impacted by the disease. Researchers are hopeful that with further testing and approval, the vaccine could be widely distributed within the next two years.\n","\n","Original Headline:  New Vaccine Shows Promise in Combating Malaria\n","Generated Headline:  malaria vaccine could reduce the catches fire by serum institute\n"]}],"source":["article=\"\"\"A new malaria vaccine, developed by researchers at the University of Oxford, has shown an efficacy rate of 77% in recent trials. This breakthrough could significantly reduce the global burden of malaria, which affects millions annually. The vaccine, named R21/Matrix-M, is expected to be a game-changer in regions heavily impacted by the disease. Researchers are hopeful that with further testing and approval, the vaccine could be widely distributed within the next two years.\"\"\"\n","\n","print(\"Article: \", article)\n","print(\"\\nOriginal Headline:  New Vaccine Shows Promise in Combating Malaria\")\n","print(\"Generated Headline: \",summarize(article))"]},{"cell_type":"code","execution_count":75,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2003,"status":"ok","timestamp":1721728358374,"user":{"displayName":"Vijoya Guha Thakurta","userId":"15815212539054957148"},"user_tz":-330},"id":"RysqZjp0-Cox","outputId":"0eb20ea1-6f12-4ad8-ee13-d1e93cc43cb6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Article:  A groundbreaking AI technology developed by MedTech Innovations has significantly improved diagnostic accuracy in healthcare. The AI system, which analyzes medical images, has achieved a 95% accuracy rate in detecting early-stage cancers. This advancement promises to enhance patient outcomes and reduce diagnostic errors. The technology is currently being tested in several hospitals and is expected to be widely adopted within the next year.\n","\n","Original Headline:  AI Technology Revolutionizes Healthcare Diagnostics\n","Generated Headline:  ai tool to reduce your infection of patient\n"]}],"source":["article=\"\"\"A groundbreaking AI technology developed by MedTech Innovations has significantly improved diagnostic accuracy in healthcare. The AI system, which analyzes medical images, has achieved a 95% accuracy rate in detecting early-stage cancers. This advancement promises to enhance patient outcomes and reduce diagnostic errors. The technology is currently being tested in several hospitals and is expected to be widely adopted within the next year.\"\"\"\n","\n","print(\"Article: \", article)\n","print(\"\\nOriginal Headline:  AI Technology Revolutionizes Healthcare Diagnostics\")\n","print(\"Generated Headline: \",summarize(article))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nDsMNPGA_8r4"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["Fz8yQwwPM3xI","twghru39kcZp","RfY0GKbWaw4G","L8GaaXrLkhkE","a13apo9BM3xT","2V5nLbGjmiYi","LXNK1lG1oaQt","Z-xv1JSyt40i","9cR8Dztwt_MV","FF15c5Q-uxj2","MGK2SAdExIlL","RPSPYQJbxUik","rjx70tbr3Jhl","i3od6q8K3fEh","FMNYzSQe3oTk","syM4Qa5sM3xY","d6AbOLId31QX","vr5ymYPz4Eso","UNCRgx4B4MCj","0Xj7rJpU4TUz","sdJXdFETc7mt"],"gpuType":"T4","provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/notebook3f06fea450-1171084a-85c3-4234-92a1-dc1d15412909.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20240715/auto/storage/goog4_request&X-Goog-Date=20240715T162154Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=b9388520635f8c387c72b3b3d12d0497a189dc2913f7892fb11b3849f239561394bbb06739d185c93ddb9970b0429c593bb57c9657521c592dc29a660c38b325c116902c5d97f592f84b793ad80747e28a8263eac19a7c9d22b15092921b97761043299ae910ea14571f6e403a5360bd5040e2eb3ef008198ebdcb20c0166027fe4b380cc76c06720529a3a31ccb0ad876a26ac85e9c0258ec6437ae09846378aaeef3007d0e039c51dd87f2fcda9614050092b10d2230a577e143229647431bddfa2981acc7590b1656014b3ce13a4d8c9a758d2927a9caa737031c997c38300cc29242edd85b21956c0d94f393cae4ce2a531aacf70ff98c578ae9925aeb5b","timestamp":1721068263452}]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":33526,"sourceId":44284,"sourceType":"datasetVersion"}],"dockerImageVersionId":30096,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":0}
